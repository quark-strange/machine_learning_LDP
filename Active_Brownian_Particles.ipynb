{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating LDP for the entropy production in Active Brownian Particle Systems\n",
    "Consider the active Brownian particle system:\n",
    "$$d\\pmb{r}_i = -\\nabla U(\\pmb{r}) + v\\pmb{b}_i + \\sqrt{2D_t} dW_t$$\n",
    "where $U$ is the purely repulsive WCA potential, $v$ is the magnitude of self-propulsion force, $\\pmb{b}$ is unit vector which direction is diffusive, and $W_t$ is a Wiener process.\n",
    "The dynamical observable of interest is the average entropy production per particle:\n",
    "$$s = \\frac{1}{NT}\\sum_{i=1}^N\\int_0^T v\\pmb{b}_i D_t^{-1}\\circ d\\pmb{r}_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import torchsde\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a DeepRitz block: f_i(s) = ϕ(Wi2 . ϕ(Wi1 . s +bi1) + bi2) + s\n",
    "class DeepRitz_block(nn.Module):\n",
    "    def __init__(self, h_size):\n",
    "        super(DeepRitz_block, self).__init__()\n",
    "        self.dim_h = h_size\n",
    "\n",
    "        self.activation_function = nn.Tanh()\n",
    "        block = [nn.Linear(self.dim_h, self.dim_h),\n",
    "                 self.activation_function,\n",
    "                 nn.Linear(self.dim_h, self.dim_h),\n",
    "                 self.activation_function]\n",
    "        self._block = nn.Sequential(*block)\n",
    "    def forward(self, x):\n",
    "        return self._block(x) + x\n",
    "\n",
    "# defining the neural network constructed by DeepRitz blocks\n",
    "class Neural_Network(nn.Module):\n",
    "    def __init__(self, in_size, h_size = 10, block_size = 1, unit_size = 10., dev=\"cpu\"):\n",
    "        super(Neural_Network, self).__init__()\n",
    "        self.num_blocks = block_size\n",
    "        self.dim_x = in_size\n",
    "        self.dim_h = h_size\n",
    "        self.unit_size = unit_size\n",
    "        self.dev = dev\n",
    "        self.dim_in = 1 + self.dim_x * 2\n",
    "        # assemble the neural network with DeepRitz blocks\n",
    "        self._block = DeepRitz_block(self.dim_h)\n",
    "        if self.dim_h > self.dim_in:\n",
    "            model = [nn.ConstantPad1d((0, self.dim_h - self.dim_in), 0)]\n",
    "        else:\n",
    "            model = [nn.Linear(self.dim_in, self.dim_h)]\n",
    "        for _ in range(self.num_blocks):\n",
    "            model.append(self._block)\n",
    "        model.append(nn.Linear(self.dim_h, 2*self.dim_x))\n",
    "        self._model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, dX1, dX2, bias):\n",
    "        y = torch.cat([dX1,\n",
    "                       dX2,\n",
    "                       bias], -1)\n",
    "        return self._model(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the dynamical system of interests: dX_t = F(X_t)dt + \\sqrt{2\\epsilon}dW_t\n",
    "# active Brownian particles\n",
    "class ODE(nn.Module):\n",
    "    def __init__(self, num_particles, v, unit_size, dev):\n",
    "        super().__init__()\n",
    "        self.epsilon = 1.\n",
    "        self.dim_r = num_particles\n",
    "        self.unit_size = unit_size\n",
    "        self.v = v\n",
    "\n",
    "        self.corrector = 2**(1/3)*torch.eye(num_particles).to(dev)\n",
    "\n",
    "    def F(self, theta, dX1, dX2):\n",
    "        dis = dX1**2 + dX2**2 + self.corrector.expand([*theta.shape[:-1], self.dim_r,self.dim_r])\n",
    "        \n",
    "        dU = torch.relu(dis**(-13/2) - 1/2 * dis**(-7/2))\n",
    "        \n",
    "        F1 = 48.*self.epsilon * (dU * dX1).sum(-1) + self.v * torch.cos(theta)\n",
    "        F2 = 48.*self.epsilon * (dU * dX2).sum(-1) + self.v * torch.sin(theta)\n",
    "        \n",
    "        return torch.cat([F1, F2, torch.zeros(F1.shape).to(dev)], -1)\n",
    "    \n",
    "    def forward(self, theta, dX1, dX2):\n",
    "        return self.F(theta, dX1, dX2)\n",
    "    \n",
    "# defining the SDE with trial driven force u(x) and fixed diffusion matrix D\n",
    "class SDE(nn.Module):\n",
    "    sde_type = 'stratonovich'\n",
    "#     sde_type = 'ito'\n",
    "    noise_type = 'general'\n",
    "\n",
    "    def __init__(self, Drift, Diffusion, unit_size=10., dev = \"cpu\"):\n",
    "        super(SDE, self).__init__()\n",
    "        self.dev = dev\n",
    "        self.dim_x = Diffusion.size(1)                      # dimensions of SDE (3 * number of particles)\n",
    "        self.unit_size = unit_size                          # length of the simulation box\n",
    "        self.dim_x_ABP = int(2/3*self.dim_x)                # dimensions of NN output (2 * number of particles)\n",
    "        self.dim_r = self.dim_x//3                          # number of particles\n",
    "\n",
    "        # (uncontrolled) drift & const diffusion matrix D:\n",
    "        self._drift_0 = Drift.to(dev)\n",
    "        self._diffusion = Diffusion.to(dev)\n",
    "        \n",
    "    # Minimum Imaging Convention\n",
    "    def MIC(self, x):\n",
    "        return (x + self.unit_size/2) % self.unit_size - self.unit_size/2\n",
    "    \n",
    "    # Periodic Boundary Condition\n",
    "    def PBC(self, x):\n",
    "        x[...,:self.dim_x_ABP] = x[...,:self.dim_x_ABP] % self.unit_size\n",
    "        x[...,self.dim_x_ABP:] = x[...,self.dim_x_ABP:] % (2 * np.pi)\n",
    "        return x\n",
    "    \n",
    "    # Input of neural network: substract the position of each particle from the 1st one to avoid boundary effects\n",
    "    def NN_input(self, x):\n",
    "        return sde.MIC(x - x[...,:1])\n",
    "    \n",
    "    # the control force u(x)\n",
    "    def drift(self, t, x):\n",
    "        x = self.PBC(x)\n",
    "        dX1 = x[...,:self.dim_r].unsqueeze(-1).expand(-1,-1,self.dim_r)\n",
    "        dX2 = x[...,self.dim_r:2*self.dim_r].unsqueeze(-1).expand(-1,-1,self.dim_r)\n",
    "        dX1 = self.MIC(dX1-dX1.transpose(-1,-2))\n",
    "        dX2 = self.MIC(dX2-dX2.transpose(-1,-2))\n",
    "        \n",
    "        # δu(x)\n",
    "        F = torch.cat([neural_network(self.NN_input(x[...,:self.dim_r]),\n",
    "                                      self.NN_input(x[...,self.dim_r:2*self.dim_r]),\n",
    "                                      biasing_field), \n",
    "                       torch.zeros([*dX1.shape[:-2], self.dim_r]).to(dev)], -1)\n",
    "        \n",
    "        # uncontrolled drift\n",
    "        F0 = self._drift_0(x[...,self.dim_x_ABP:], dX1, dX2)\n",
    "        \n",
    "        return F + F0\n",
    "\n",
    "    # the diffusion matrix\n",
    "    def diffusion(self, t, x):\n",
    "        return self._diffusion.expand(x.size(0), self.dim_x, self.dim_x)\n",
    "\n",
    "    # the F(x)\n",
    "    def drift_0(self, t, x):\n",
    "        x = self.PBC(x)\n",
    "        dX1 = x[...,:self.dim_r].unsqueeze(-1).expand(-1,-1,self.dim_r)\n",
    "        dX2 = x[...,self.dim_r:2*self.dim_r].unsqueeze(-1).expand(-1,-1,self.dim_r)\n",
    "        dX1 = self.MIC(dX1-dX1.transpose(-1,-2))\n",
    "        dX2 = self.MIC(dX2-dX2.transpose(-1,-2))\n",
    "        return self._drift_0(x[...,2*self.dim_r:], dX1, dX2)\n",
    "    \n",
    "def replica_exchange(traj, x_init):\n",
    "    swaps = batch_size1\n",
    "    with torch.no_grad():\n",
    "        swap = [np.random.choice(batch_size0, swaps, replace=False), \n",
    "                np.random.choice(batch_size1, swaps, replace=False)+batch_size0]\n",
    "        M_T0 = torch.sum((sde.MIC(traj[1:,swap[0],:2*dim_x]-traj[:-1,swap[0],:2*dim_x])/dt-(F+F0)[:,swap[0]])**2, (0,-1))\n",
    "        M_T1 = torch.sum((sde.MIC(traj[1:,swap[1],:2*dim_x]-traj[:-1,swap[1],:2*dim_x])/dt-(F+F0)[:,swap[1]])**2, (0,-1))\n",
    "        M_T = -(M_T0 - M_T1).cpu().detach().numpy() * dt /4\n",
    "        j = np.argwhere(np.random.rand(swaps) < np.exp(M_T)).T[0]\n",
    "        x_init[[*swap[0][j],*swap[1][j]]] = x_init[[*swap[1][j],*swap[0][j]]]\n",
    "        print('%i replica swapped' % j.size)\n",
    "    return x_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_x, dim_h, num_blocks = 40, 1000, 6\n",
    "Dt, v = 1, 10\n",
    "unit_size = np.sqrt(dim_x / 0.1) + 0.0001\n",
    "\n",
    "dev = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(dev)\n",
    "\n",
    "neural_network = Neural_Network(dim_x, dim_h, num_blocks, unit_size, dev).to(dev)\n",
    "\n",
    "Dt = 1.\n",
    "Dr = 3.*Dt\n",
    "diffusion = torch.tensor( np.sqrt(2 * np.kron(np.diag([Dt, Dt, Dr]), np.eye(dim_x))) ).float().to(dev)\n",
    "\n",
    "ode = ODE(dim_x, v, unit_size, dev)\n",
    "sde = SDE(ode, diffusion, unit_size, dev).to(dev)\n",
    "if sde.sde_type == 'ito':\n",
    "    sde_method = 'euler'\n",
    "else:\n",
    "    sde_method = 'midpoint'\n",
    "Psi = []\n",
    "A_T = []\n",
    "K_T = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 1e-4\n",
    "T = 1000 * dt\n",
    "\n",
    "batch_size0, batch_size1 = 50, 50\n",
    "batch_size = batch_size0 + batch_size1\n",
    "LR = 1e-2\n",
    "Bias0 = -0.04\n",
    "Bias1 = -0.05\n",
    "biasing_field = torch.cat([Bias0 * torch.ones([batch_size0, 1]).to(dev), \n",
    "                           Bias1 * torch.ones([batch_size1, 1]).to(dev)])\n",
    "\n",
    "x0 = torch.cat([torch.rand([batch_size, dim_x*2]) * unit_size, torch.rand([batch_size, dim_x]) * 2 * np.pi],-1).to(dev)\n",
    "with torch.no_grad():\n",
    "    x_init = torchsde.sdeint(sde, x0.to(dev), torch.arange(0, .1+dt, dt).to(dev), dt = dt, \n",
    "                         method=sde_method, names={'drift': 'drift', 'diffusion': 'diffusion'})[-1]\n",
    "optimizer = torch.optim.Adadelta(neural_network.parameters(), lr = LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = torch.arange(0, T+dt, dt).to(dev)\n",
    "normalisation = dim_x * T\n",
    "\n",
    "step = 0\n",
    "while step < 10000:\n",
    "    start_time = time.time()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Generating a batch of trajectories\n",
    "    with torch.no_grad():\n",
    "        traj = torchsde.sdeint(sde, x_init.to(dev), ts, dt = dt,\n",
    "                               method=sde_method, names={'drift': 'drift', 'diffusion': 'diffusion'})\n",
    "    \n",
    "    ###################################################################################################\n",
    "    ## Computing K_T & A_T silmutaneously over T & batch:\n",
    "        dX1 = traj[...,:dim_x].unsqueeze(-1).expand(*traj.shape[:-1],dim_x, dim_x)\n",
    "        dX2 = traj[...,dim_x:2*dim_x].unsqueeze(-1).expand(*traj.shape[:-1],dim_x, dim_x)\n",
    "        dX1 = sde.MIC(dX1-dX1.transpose(-1,-2))\n",
    "        dX2 = sde.MIC(dX2-dX2.transpose(-1,-2))\n",
    "    \n",
    "    F0 = ode(traj[...,2*dim_x:], dX1, dX2)[:-1,:,:2*dim_x]\n",
    "    F = neural_network(sde.NN_input(traj[...,:dim_x]),\n",
    "                       sde.NN_input(traj[...,dim_x:2*dim_x]),\n",
    "                       biasing_field.expand([*traj.shape[:-1],1]))[:-1]\n",
    "    dX = (F + F0) * dt\n",
    "    b = torch.cat([torch.cos(traj[:,:,2*dim_x:]), torch.sin(traj[:,:,2*dim_x:])], 2)\n",
    "    \n",
    "    K = torch.sum(F**2, (0,-1))/Dt/2*dt/normalisation\n",
    "    A = v/Dt * torch.sum((b[1:]+b[:-1])/2 * dX, (0,-1))/normalisation\n",
    "    ###################################################################################################\n",
    "    \n",
    "    ###################################################################################################\n",
    "    ## Alternative approach: computing K_T & A_T step by step to save memory:\n",
    "    \n",
    "#     K = torch.zeros(batch_size).to(dev)\n",
    "#     A = torch.zeros(batch_size).to(dev)\n",
    "#     for i in range(traj.shape[0]):\n",
    "#         dX1 = traj[i:i+1,:,:dim_x].unsqueeze(-1).expand(-1,-1,dim_x)\n",
    "#         dX2 = traj[i:i+1,:,dim_x:2*dim_x].unsqueeze(-1).expand(-1,-1,dim_x)\n",
    "#         dX1 = sde.MIC(dX1-dX1.transpose(-1,-2))\n",
    "#         dX2 = sde.MIC(dX2-dX2.transpose(-1,-2))\n",
    "#\n",
    "#         F0 = ode(traj[i:i+1,:,2*dim_x:], dX1, dX2)[...,:2*dim_x]    \n",
    "#         F = neural_network(sde.NN_input(traj[i:i+1,:,:dim_x]),\n",
    "#                            sde.NN_input(traj[i:i+1,:,dim_x:2*dim_x]),\n",
    "#                            biasing_field[None])\n",
    "#         dX = (F + F0) * dt\n",
    "#         b = torch.cat([torch.cos(traj[i:i+1,:,2*dim_x:]), torch.sin(traj[i:i+1,:,2*dim_x:])], 2)\n",
    "#\n",
    "#         K = K + torch.sum(F**2, (0,-1))/Dt/2*dt/normalisation\n",
    "#         A = A + v/Dt * torch.sum(b * dX, (0,-1))/normalisation\n",
    "    ###################################################################################################\n",
    "    \n",
    "    ## Computing the Lagrangian:\n",
    "    loss_batch = (K - biasing_field.squeeze() * dim_x * A)\n",
    "    loss = loss_batch.mean()\n",
    "\n",
    "    assert not torch.isnan(loss), \"We've got a NaN\"\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    step += 1\n",
    "    \n",
    "    Psi.append(-loss_batch.cpu().detach().numpy()/dim_x)\n",
    "    K_T.append(K.cpu().detach().numpy())\n",
    "    A_T.append(A.cpu().detach().numpy())\n",
    "    x_init = traj[-1].detach()    \n",
    "    # Replica Exchange\n",
    "    if Bias0 != Bias1:\n",
    "        x_init = replica_exchange(traj, x_init)\n",
    "        \n",
    "    t_simul = time.time()\n",
    "    print('step %i - time: %.2f sec - SCGF: %.4f * %.4f - %.4f = %.4f' \n",
    "            % (step, float(t_simul-start_time), biasing_field[0], \n",
    "               np.mean(A_T[-1][:batch_size0]), np.mean(K_T[-1][:batch_size0]), np.mean(Psi[-1][:batch_size0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,3))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(np.array(Psi)[10:,:batch_size0].mean(1))\n",
    "plt.grid()\n",
    "# plt.ylim([0,5])\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('training steps')\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(np.array(K_T)[0:,:batch_size0].mean(1))\n",
    "plt.grid()\n",
    "plt.ylabel(r'$K_T$')\n",
    "plt.xlabel('training steps')\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(np.array(A_T)[0:,:batch_size0].mean(1))\n",
    "plt.grid()\n",
    "plt.ylabel(r'$A_T$')\n",
    "plt.xlabel('training steps')\n",
    "plt.show()\n",
    "\n",
    "print('SCGF =', -np.array(Psi)[-500:,:batch_size0].mean(),', EP =', np.mean(np.array(A_T)[-500:,:batch_size0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vec(x, t):\n",
    "    m = 0\n",
    "    vec = x[...,2*dim_x:]\n",
    "    dx = sde.MIC(x[t+1,0]-x[t,0])\n",
    "    markersize = 50000/np.pi/unit_size**2*4\n",
    "    ax = plt.subplot(aspect=1)\n",
    "    plt.plot([unit_size, unit_size], [0, 2*unit_size],'--', color = col[2])\n",
    "    plt.plot([0, 2*unit_size], [unit_size, unit_size],'--', color = col[2])\n",
    "    \n",
    "    plt.scatter(x[t,m,:dim_x],x[t,m,dim_x:2*dim_x],s=markersize,alpha=0.5, color='c')    \n",
    "    plt.scatter(unit_size + x[t,m,:dim_x],x[t,m,dim_x:2*dim_x],s=markersize,alpha=0.5, color='c')  \n",
    "    plt.scatter(x[t,m,:dim_x],unit_size + x[t,m,dim_x:2*dim_x],s=markersize,alpha=0.5, color='c')   \n",
    "    plt.scatter(unit_size + x[t,m,:dim_x],unit_size + x[t,m,dim_x:2*dim_x],s=markersize,alpha=0.5, color='c')\n",
    "    \n",
    "    plt.xlim([0, unit_size + unit_size])\n",
    "    plt.ylim([0, unit_size + unit_size])\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= traj.cpu().detach().numpy()\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "plot_vec(x, -2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
